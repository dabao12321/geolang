{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering Lexical Data\n",
    "\n",
    "This notebook will serve to gather vocabulary unique to a certain region of the U.S. Data will be drawn primarily from the Dictionary of American Regional English (DARE).\n",
    "\n",
    "recommended to do:\n",
    "- plug data into SVM model (sklearn Pipeline, put data into pipeline, pipeline converts to number vectors, to SVM classifier)\n",
    "- use Grid Search for tuning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Converts a string listing state abbreviations to a string listing state names.\n",
    "\n",
    "ex. \"ak, al, ar\" --> \"alaska, alabama, arkansas\"\n",
    "\n",
    "''' \n",
    "\n",
    "def abbrev_to_state(abbreviations):\n",
    "    '''Takes in a comma seperated string that lists state abbreviations.\n",
    "    Outputs a comma seperated string that lists full state names.\n",
    "    \n",
    "    Ex. The call 'abbrev_to_state(\"ak, al, ar\") will return a string:\n",
    "    \n",
    "    \"alaska, alabama, arkansas\"\n",
    "    \n",
    "    '''\n",
    "    output = \"\"\n",
    "    abbrevs = abbreviations.split(\", \")\n",
    "    states = {\n",
    "        'AK': 'Alaska',\n",
    "        'AL': 'Alabama',\n",
    "        'AR': 'Arkansas',\n",
    "        'AS': 'American Samoa',\n",
    "        'AZ': 'Arizona',\n",
    "        'CA': 'California',\n",
    "        'CO': 'Colorado',\n",
    "        'CT': 'Connecticut',\n",
    "        'DC': 'District of Columbia',\n",
    "        'DE': 'Delaware',\n",
    "        'FL': 'Florida',\n",
    "        'GA': 'Georgia',\n",
    "        'GU': 'Guam',\n",
    "        'HI': 'Hawaii',\n",
    "        'IA': 'Iowa',\n",
    "        'ID': 'Idaho',\n",
    "        'IL': 'Illinois',\n",
    "        'IN': 'Indiana',\n",
    "        'KS': 'Kansas',\n",
    "        'KY': 'Kentucky',\n",
    "        'LA': 'Louisiana',\n",
    "        'MA': 'Massachusetts',\n",
    "        'MD': 'Maryland',\n",
    "        'ME': 'Maine',\n",
    "        'MI': 'Michigan',\n",
    "        'MN': 'Minnesota',\n",
    "        'MO': 'Missouri',\n",
    "        'MP': 'Northern Mariana Islands',\n",
    "        'MS': 'Mississippi',\n",
    "        'MT': 'Montana',\n",
    "        'NA': 'National',\n",
    "        'NC': 'North Carolina',\n",
    "        'ND': 'North Dakota',\n",
    "        'NE': 'Nebraska',\n",
    "        'NH': 'New Hampshire',\n",
    "        'NJ': 'New Jersey',\n",
    "        'NM': 'New Mexico',\n",
    "        'NV': 'Nevada',\n",
    "        'NY': 'New York',\n",
    "        'OH': 'Ohio',\n",
    "        'OK': 'Oklahoma',\n",
    "        'OR': 'Oregon',\n",
    "        'PA': 'Pennsylvania',\n",
    "        'PR': 'Puerto Rico',\n",
    "        'RI': 'Rhode Island',\n",
    "        'SC': 'South Carolina',\n",
    "        'SD': 'South Dakota',\n",
    "        'TN': 'Tennessee',\n",
    "        'TX': 'Texas',\n",
    "        'UT': 'Utah',\n",
    "        'VA': 'Virginia',\n",
    "        'VI': 'Virgin Islands',\n",
    "        'VT': 'Vermont',\n",
    "        'WA': 'Washington',\n",
    "        'WI': 'Wisconsin',\n",
    "        'WV': 'West Virginia',\n",
    "        'WY': 'Wyoming'\n",
    "    }\n",
    "    for state in abbrevs:\n",
    "        name = states[state.upper()]\n",
    "        output += name.lower() + \", \"\n",
    "    return output[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary containing the states within regions\n",
    "\n",
    "regions = {\"allegheny mountains\": abbrev_to_state(\"md, va, pa, wv\"),\n",
    "                    \"appalachians\": abbrev_to_state(\"pa, wv, md, va, nc, ky, tn, nc, ga, al\"),\n",
    "                    \"atlantic\": abbrev_to_state(\"me, nh, vt, ma, ct, ri, ny, pa, nj, de, md, dc, va, nc, ga, sc, fl\"), \n",
    "                    \"central\": abbrev_to_state(\"ne, ks, mo, ok, ar\"),\n",
    "                    \"central atlantic\": abbrev_to_state(\"pa, de, nj, md, dc, va\"),\n",
    "                    \"chesapeake bay\": abbrev_to_state(\"md, va\"),\n",
    "                    \"delmarva\": abbrev_to_state(\"de, md, va\"),\n",
    "                    \"desert southwest\": abbrev_to_state(\"ca, az, nm\"),\n",
    "                    \"great lakes\": abbrev_to_state(\"mn, wi, mi, il, in, oh, pa, ny\"),\n",
    "                    \"gulf states\": abbrev_to_state(\"tx, la, ms, al, fl\"),\n",
    "                    \"inland north\": abbrev_to_state(\"wa, or, id, mt, wy, nd, sd, mn, ia, wi, il, mi, in, oh, pa, ny, nj\"),\n",
    "                    \"inland south\": abbrev_to_state(\"ky, tn, al, ms\"),\n",
    "                    \"lower mississippi valley\": abbrev_to_state(\"mo, ky, il, tn, ar, la, ms\"),\n",
    "                    \"middle atlantic\": abbrev_to_state(\"md, dc, va, nc, sc\"),\n",
    "                    \"midland\": abbrev_to_state(\"sd, ne, ia, mo, ok, ar, la, ms, tn, ky, il, in, oh, wv, pa, md, de, nj, va, nc, ga, al, sc\"),\n",
    "                    \"mississippi valley\": abbrev_to_state(\"mn, wi, ia, il, mo, ar, ky, tn, la, ms\"),\n",
    "                    \"mississippi ohio valleys\": abbrev_to_state(\"mn, wi, ia, il, mo, in, oh, ky\"),\n",
    "                    \"new england\": abbrev_to_state(\"vt, nh, me, ma, ct, ri\"),\n",
    "                    \"north\": abbrev_to_state(\"wa, or, id, mt, wy, nd, sd, mn, wi, ia, il, mi, in, oh, pa, ny, nj, ct, vt, nh, ma, ri, me\"),\n",
    "                    \"north atlantic\": abbrev_to_state(\"me, nh, vt, ny, nj, ma, ct, ri\"),\n",
    "                    \"north central\": abbrev_to_state(\"wi, mi, il, in, oh, ky\"),\n",
    "                    \"north midland\": abbrev_to_state(\"ne, sd, ia, mo, il, in, oh, wv, pa, md, de, nj\"),\n",
    "                    \"northeast\": abbrev_to_state(\"pa, nj, ny, vt, nh, ma, ct, me, ri\"),\n",
    "                    \"northwest\": abbrev_to_state(\"wa, or, id, mt, wy\"),\n",
    "                    \"ohio valley\": abbrev_to_state(\"il, mo, ky, in, oh\"),\n",
    "                    \"okefenokee\": abbrev_to_state(\"ga, fl\"),\n",
    "                    \"ozarks\": abbrev_to_state(\"mo, ok, ar\"),\n",
    "                    \"pacific\": abbrev_to_state(\"wa, or, ca\"),\n",
    "                    \"pacific northwest\": abbrev_to_state(\"wa, or, ca\"),\n",
    "                    \"plains states\": abbrev_to_state(\"co, ks, ne\"),\n",
    "                    \"rocky mountains\": abbrev_to_state(\"id, mt, wy, co, ut, nv\"),\n",
    "                    \"smoky mountains\": abbrev_to_state(\"tn, nc\"),\n",
    "                    \"south\": abbrev_to_state(\"tx, la, ms, al, fl, ga, nc, sc, va, dc, md\"),\n",
    "                    \"south atlantic\": abbrev_to_state(\"nc, ga, sc, fl\"),\n",
    "                    \"south midland\": abbrev_to_state(\"ok, mo, ar, la, ms, il, in, ky, tn, al, oh, wv, va, nc, ga, dc, md, de, sc\"),\n",
    "                    \"southeast\": abbrev_to_state(\"ms, tn, al, nc, ga, fl, sc\"),\n",
    "                    \"southwest\": abbrev_to_state(\"az, nm, tx, ca, ok\"),\n",
    "                    \"upper midwest\": abbrev_to_state(\"nd, sd, ne, mn, ia\"),\n",
    "                    \"upper mississippi valley\": abbrev_to_state(\"mn, ia, mo, wi, il\"),\n",
    "                    \"west\": abbrev_to_state(\"wa, or, ca, nv, az, id, ut, nm, mt, wy, co, ok, tx, nd, sd, ne, ks, ok\"),\n",
    "                    \"west midland\": abbrev_to_state(\"sd, ne, ia, mo, ok, ar, la, ms, il, in, oh, ky, tn, al, wv, va, nc, sc\")\n",
    "                   }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand_region(region, word):\n",
    "    '''This expands a singular entry of a general region to multiple entries containing\n",
    "    each state within the region. Takes in two string entries and outputs a pandas DataFrame\n",
    "    \n",
    "    Ex: The call 'print(expand_region('pacific', 'hi'))' will return a DataFrame:\n",
    "    \n",
    "        dialect word\n",
    "    0  washington   hi\n",
    "    1      oregon   hi\n",
    "    2  california   hi  \n",
    "    \n",
    "    '''\n",
    "    states = regions[region].split(\", \")\n",
    "    toadd = pd.DataFrame()\n",
    "    toadd['dialect'] = states\n",
    "    toadd['word'] = word\n",
    "    return toadd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Seperating all regions in U.S. into state contained areas\n",
    "# one row per word corresponding to a single state (not region)\n",
    "alldata = pd.read_json(\"../data/geodare.json\")\n",
    "alldata = alldata.drop(\"dialect subregions\", axis=1)\n",
    "todrop = []\n",
    "for index, row in alldata.iterrows():\n",
    "    if row[\"dialect\"] in regions.keys():\n",
    "        toadd = expand_region(row['dialect'], row['word'])\n",
    "        alldata = alldata.append(toadd)\n",
    "        todrop.append(index)\n",
    "        \n",
    "alldata = alldata.drop(todrop, axis = 0)\n",
    "alldata = alldata.reset_index()\n",
    "for index, row in alldata.iterrows():\n",
    "    alldata.set_value(index, 'index', index)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Done \n",
    "\n",
    "Now the data is much simpler to work with, and we can continuously add to the dataset from other sources and with more substantial data, i.e. phrases and sentences rather than words. For now, let's export this to our data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export the data to a .csv file\n",
    "alldata.to_csv(\"../data/cleaned_dare_corpus.csv\", sep = \",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
