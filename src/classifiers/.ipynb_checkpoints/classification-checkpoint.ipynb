{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlating Language to Geographic Location\n",
    "\n",
    "After parsing a significant amount of geographically-linked phrases, now we'd like a way to predict the geographic origin of a speaker or writer via their language. Since no dataset can possibly cover the extent of U.S. dialects, we ought to utilize certain machine learning techniques to predict geographic origin even when the raw data is not entirely conclusive.\n",
    "\n",
    "In order to accomplish this, we will first vectorize words and sentences, then utilize a Naive Bayes classifier (scikit-learn's implementation).\n",
    "\n",
    "Please refer to and run `geodare.json`, which converts the raw DARE corpus into a more usable format, prior to using this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For DARE corpus only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Read in the cleaned DARE corpus\n",
    "'''\n",
    "geodata = pd.read_csv(\"../../data/cleaned_dare_corpus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alabama', 'alaska', 'algonquian', 'arizona', 'arkansas', 'bahamian', 'california', 'caribbean', 'cherokee', 'choctaw', 'colorado', 'connecticut', 'delaware', 'florida', 'georgia', 'hawaii', 'idaho', 'illinois', 'illinois, chicago', 'indiana', 'iowa', 'kansas', 'kentucky', 'louisiana', 'louisiana, new orleans', 'maine', 'maryland', 'maryland, baltimore', 'massachusetts', 'massachusetts, boston', 'massachusetts, cape cod', 'massachusetts, nantucket', 'michigan', 'minnesota', 'mississippi', 'missouri', 'montana', 'narraganset', 'nebraska', 'nevada', 'new hampshire', 'new jersey', 'new mexico', 'new york', 'new york city', 'new york, hudson valley', 'new york, long island', 'new york, upstate', 'newfoundland', 'north carolina', 'north dakota', 'ohio', 'oklahoma', 'oregon', 'pennsylvania', 'rhode island', 'south carolina', 'south carolina, charleston', 'south dakota', 'tennessee', 'texas', 'utah', 'vermont', 'virginia', 'washington', 'washington, dc', 'west virginia', 'wisconsin', 'wyoming', 'district of columbia']\n"
     ]
    }
   ],
   "source": [
    "'''Create classification categories, i.e. target names\n",
    "'''\n",
    "catagories = []\n",
    "for x in geodata['dialect']:\n",
    "    if x not in catagories:\n",
    "        catagories.append(x)\n",
    "print(catagories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create data point and target lists\n",
    "'''\n",
    "examples = []\n",
    "targets = []\n",
    "\n",
    "for i,x in enumerate(geodata['word']):\n",
    "    if pd.notnull(x):\n",
    "        examples.append(x)\n",
    "        dialect = geodata.get_value(i,'dialect')\n",
    "        target = catagories.index(dialect)\n",
    "        targets.append(target) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For SB translations only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' !!! IMP: TEMPORARY. IMPORT METHOD FROM lexicon.ipynb ONCE CONVERTED INTO PYTHON SCRIPT\n",
    "\n",
    "Converts a string listing state abbreviations to a string listing state names.\n",
    "ex. \"ak, al, ar\" --> \"alaska, alabama, arkansas\"\n",
    "\n",
    "''' \n",
    "\n",
    "def abbrev_to_state(abbreviations):\n",
    "    '''Takes in a comma seperated string that lists state abbreviations.\n",
    "    Outputs a comma seperated string that lists full state names.\n",
    "    \n",
    "    Ex. The call 'abbrev_to_state(\"ak, al, ar\") will return a string:\n",
    "    \n",
    "    \"alaska, alabama, arkansas\"\n",
    "    \n",
    "    '''\n",
    "    output = \"\"\n",
    "    abbrevs = abbreviations.split(\", \")\n",
    "    states = {\n",
    "        'AK': 'Alaska',\n",
    "        'AL': 'Alabama',\n",
    "        'AR': 'Arkansas',\n",
    "        'AS': 'American Samoa',\n",
    "        'AZ': 'Arizona',\n",
    "        'CA': 'California',\n",
    "        'CO': 'Colorado',\n",
    "        'CT': 'Connecticut',\n",
    "        'DC': 'District of Columbia',\n",
    "        'DE': 'Delaware',\n",
    "        'FL': 'Florida',\n",
    "        'GA': 'Georgia',\n",
    "        'GU': 'Guam',\n",
    "        'HI': 'Hawaii',\n",
    "        'IA': 'Iowa',\n",
    "        'ID': 'Idaho',\n",
    "        'IL': 'Illinois',\n",
    "        'IN': 'Indiana',\n",
    "        'KS': 'Kansas',\n",
    "        'KY': 'Kentucky',\n",
    "        'LA': 'Louisiana',\n",
    "        'MA': 'Massachusetts',\n",
    "        'MD': 'Maryland',\n",
    "        'ME': 'Maine',\n",
    "        'MI': 'Michigan',\n",
    "        'MN': 'Minnesota',\n",
    "        'MO': 'Missouri',\n",
    "        'MP': 'Northern Mariana Islands',\n",
    "        'MS': 'Mississippi',\n",
    "        'MT': 'Montana',\n",
    "        'NA': 'National',\n",
    "        'NC': 'North Carolina',\n",
    "        'ND': 'North Dakota',\n",
    "        'NE': 'Nebraska',\n",
    "        'NH': 'New Hampshire',\n",
    "        'NJ': 'New Jersey',\n",
    "        'NM': 'New Mexico',\n",
    "        'NV': 'Nevada',\n",
    "        'NY': 'New York',\n",
    "        'OH': 'Ohio',\n",
    "        'OK': 'Oklahoma',\n",
    "        'OR': 'Oregon',\n",
    "        'PA': 'Pennsylvania',\n",
    "        'PR': 'Puerto Rico',\n",
    "        'RI': 'Rhode Island',\n",
    "        'SC': 'South Carolina',\n",
    "        'SD': 'South Dakota',\n",
    "        'TN': 'Tennessee',\n",
    "        'TX': 'Texas',\n",
    "        'UT': 'Utah',\n",
    "        'VA': 'Virginia',\n",
    "        'VI': 'Virgin Islands',\n",
    "        'VT': 'Vermont',\n",
    "        'WA': 'Washington',\n",
    "        'WI': 'Wisconsin',\n",
    "        'WV': 'West Virginia',\n",
    "        'WY': 'Wyoming'\n",
    "    }\n",
    "    for state in abbrevs:\n",
    "        name = states[state.upper()]\n",
    "        output += name.lower() + \", \"\n",
    "    return output[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialect</th>\n",
       "      <th>name</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CA</td>\n",
       "      <td>LENORE</td>\n",
       "      <td>So you don't need to go borrow equipment from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MT</td>\n",
       "      <td>LYNNE</td>\n",
       "      <td>H YWN Well we're gonna have to find somewhere ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MT</td>\n",
       "      <td>DORIS</td>\n",
       "      <td>So Mae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MT</td>\n",
       "      <td>LYNNE</td>\n",
       "      <td>I'm gonna Hx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MT</td>\n",
       "      <td>DORIS</td>\n",
       "      <td>Mae Lynne XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MT</td>\n",
       "      <td>LYNNE</td>\n",
       "      <td>H We're not gonna do the feet today I'm gonna ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CA</td>\n",
       "      <td>LENORE</td>\n",
       "      <td>Did they train you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MT</td>\n",
       "      <td>LYNNE</td>\n",
       "      <td>yeah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CA</td>\n",
       "      <td>LENORE</td>\n",
       "      <td>Did they train you that XX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MT</td>\n",
       "      <td>LYNNE</td>\n",
       "      <td>yeah yeah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CA</td>\n",
       "      <td>LENORE</td>\n",
       "      <td>So you have your own equipment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MT</td>\n",
       "      <td>LYNNE</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CA</td>\n",
       "      <td>LENORE</td>\n",
       "      <td>but</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MT</td>\n",
       "      <td>LYNNE</td>\n",
       "      <td>TSK H No I don't have my own equipment at all ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CA</td>\n",
       "      <td>LENORE</td>\n",
       "      <td>yeah I bet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MT</td>\n",
       "      <td>LYNNE</td>\n",
       "      <td>H and X then X so we had to know these tendons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CA</td>\n",
       "      <td>LENORE</td>\n",
       "      <td>mhm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MT</td>\n",
       "      <td>LYNNE</td>\n",
       "      <td>and that's where you kinda SM H kinda need a l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CA</td>\n",
       "      <td>LENORE</td>\n",
       "      <td>really</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MT</td>\n",
       "      <td>LYNNE</td>\n",
       "      <td>Well you can trim em too short H And make em y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>CA</td>\n",
       "      <td>LENORE</td>\n",
       "      <td>THROAT THROAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MT</td>\n",
       "      <td>LYNNE</td>\n",
       "      <td>on a horse H And that's as far as we got I mea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>CA</td>\n",
       "      <td>LENORE</td>\n",
       "      <td>farrier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>MT</td>\n",
       "      <td>LYNNE</td>\n",
       "      <td>H th Yeah farrier is what they're called H And...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>CA</td>\n",
       "      <td>LENORE</td>\n",
       "      <td>You said you never made the horseshoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>MT</td>\n",
       "      <td>LYNNE</td>\n",
       "      <td>gonna say P&gt;PAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>CA</td>\n",
       "      <td>LENORE</td>\n",
       "      <td>but</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>MT</td>\n",
       "      <td>LYNNE</td>\n",
       "      <td>H Well w um when we put em on a horse's hoof a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>CA</td>\n",
       "      <td>LENORE</td>\n",
       "      <td>So every every uh horseshoe is made custom-mad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>MT</td>\n",
       "      <td>LYNNE</td>\n",
       "      <td>H No no H No What What we do then that's that'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27955</th>\n",
       "      <td>WI</td>\n",
       "      <td>CAM</td>\n",
       "      <td>&lt;MRC Oh geez Fred MRC&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27956</th>\n",
       "      <td>WI</td>\n",
       "      <td>JO</td>\n",
       "      <td>Frederick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27957</th>\n",
       "      <td>WI</td>\n",
       "      <td>WESS</td>\n",
       "      <td>You can pull them yourself with a finger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27958</th>\n",
       "      <td>WI</td>\n",
       "      <td>JO</td>\n",
       "      <td>H VOX Yes VOX They're easy But not ones that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27959</th>\n",
       "      <td>WI</td>\n",
       "      <td>CAM</td>\n",
       "      <td>X Well X All little kids pull their own teeth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27960</th>\n",
       "      <td>WI</td>\n",
       "      <td>JO</td>\n",
       "      <td>Yeah cause you wanted the dollar Hx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27961</th>\n",
       "      <td>WI</td>\n",
       "      <td>CAM</td>\n",
       "      <td>Oh I don't think it was a dollar to start with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27962</th>\n",
       "      <td>WI</td>\n",
       "      <td>JO</td>\n",
       "      <td>H Well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27963</th>\n",
       "      <td>WI</td>\n",
       "      <td>WESS</td>\n",
       "      <td>What was it that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27964</th>\n",
       "      <td>WI</td>\n",
       "      <td>JO</td>\n",
       "      <td>Not well maybe when not when you started</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27965</th>\n",
       "      <td>WI</td>\n",
       "      <td>WESS</td>\n",
       "      <td>They used They used to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27966</th>\n",
       "      <td>WI</td>\n",
       "      <td>JO</td>\n",
       "      <td>It got up there</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27967</th>\n",
       "      <td>WI</td>\n",
       "      <td>CAM</td>\n",
       "      <td>I don't remember</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27968</th>\n",
       "      <td>WI</td>\n",
       "      <td>WESS</td>\n",
       "      <td>Tie a string around your tooth and then close ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27969</th>\n",
       "      <td>WI</td>\n",
       "      <td>JO</td>\n",
       "      <td>Hx H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27970</th>\n",
       "      <td>WI</td>\n",
       "      <td>CAM</td>\n",
       "      <td>Oh I think that was just a joke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27971</th>\n",
       "      <td>WI</td>\n",
       "      <td>JO</td>\n",
       "      <td>H SM I think so SM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27972</th>\n",
       "      <td>WI</td>\n",
       "      <td>WESS</td>\n",
       "      <td>Mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27973</th>\n",
       "      <td>WI</td>\n",
       "      <td>JO</td>\n",
       "      <td>Yeah I never saw anybody do it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27974</th>\n",
       "      <td>WI</td>\n",
       "      <td>WESS</td>\n",
       "      <td>Old wive's tale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27975</th>\n",
       "      <td>WI</td>\n",
       "      <td>CAM</td>\n",
       "      <td>Yeah ENV ROCKING_CHAIR_SQUEAKING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27976</th>\n",
       "      <td>WI</td>\n",
       "      <td>JO</td>\n",
       "      <td>Aw me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27977</th>\n",
       "      <td>WI</td>\n",
       "      <td>CAM</td>\n",
       "      <td>Your chair's a little squeaky X Jo Lynne X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27978</th>\n",
       "      <td>WI</td>\n",
       "      <td>JO</td>\n",
       "      <td>GASP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27979</th>\n",
       "      <td>WI</td>\n",
       "      <td>WESS</td>\n",
       "      <td>Not a creature was stirring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27980</th>\n",
       "      <td>WI</td>\n",
       "      <td>JO</td>\n",
       "      <td>Well oil it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27981</th>\n",
       "      <td>WI</td>\n",
       "      <td>CAM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27982</th>\n",
       "      <td>WI</td>\n",
       "      <td>JO</td>\n",
       "      <td>H It's a getting to be old</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27983</th>\n",
       "      <td>MI</td>\n",
       "      <td>FRED</td>\n",
       "      <td>You did bring the gifts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27984</th>\n",
       "      <td>WI</td>\n",
       "      <td>CAM</td>\n",
       "      <td>I did bring the gifts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27985 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      dialect    name                                               word\n",
       "0          CA  LENORE  So you don't need to go borrow equipment from ...\n",
       "1          MT   LYNNE  H YWN Well we're gonna have to find somewhere ...\n",
       "2          MT   DORIS                                             So Mae\n",
       "3          MT   LYNNE                                       I'm gonna Hx\n",
       "4          MT   DORIS                                       Mae Lynne XX\n",
       "5          MT   LYNNE  H We're not gonna do the feet today I'm gonna ...\n",
       "6          CA  LENORE                                 Did they train you\n",
       "7          MT   LYNNE                                               yeah\n",
       "8          CA  LENORE                         Did they train you that XX\n",
       "9          MT   LYNNE                                          yeah yeah\n",
       "10         CA  LENORE                     So you have your own equipment\n",
       "11         MT   LYNNE                                                  H\n",
       "12         CA  LENORE                                                but\n",
       "13         MT   LYNNE  TSK H No I don't have my own equipment at all ...\n",
       "14         CA  LENORE                                         yeah I bet\n",
       "15         MT   LYNNE  H and X then X so we had to know these tendons...\n",
       "16         CA  LENORE                                                mhm\n",
       "17         MT   LYNNE  and that's where you kinda SM H kinda need a l...\n",
       "18         CA  LENORE                                             really\n",
       "19         MT   LYNNE  Well you can trim em too short H And make em y...\n",
       "20         CA  LENORE                                      THROAT THROAT\n",
       "21         MT   LYNNE  on a horse H And that's as far as we got I mea...\n",
       "22         CA  LENORE                                            farrier\n",
       "23         MT   LYNNE  H th Yeah farrier is what they're called H And...\n",
       "24         CA  LENORE             You said you never made the horseshoes\n",
       "25         MT   LYNNE                                    gonna say P>PAR\n",
       "26         CA  LENORE                                                but\n",
       "27         MT   LYNNE  H Well w um when we put em on a horse's hoof a...\n",
       "28         CA  LENORE  So every every uh horseshoe is made custom-mad...\n",
       "29         MT   LYNNE  H No no H No What What we do then that's that'...\n",
       "...       ...     ...                                                ...\n",
       "27955      WI     CAM                             <MRC Oh geez Fred MRC>\n",
       "27956      WI      JO                                          Frederick\n",
       "27957      WI    WESS           You can pull them yourself with a finger\n",
       "27958      WI      JO   H VOX Yes VOX They're easy But not ones that ...\n",
       "27959      WI     CAM  X Well X All little kids pull their own teeth ...\n",
       "27960      WI      JO                Yeah cause you wanted the dollar Hx\n",
       "27961      WI     CAM     Oh I don't think it was a dollar to start with\n",
       "27962      WI      JO                                             H Well\n",
       "27963      WI    WESS                                   What was it that\n",
       "27964      WI      JO           Not well maybe when not when you started\n",
       "27965      WI    WESS                             They used They used to\n",
       "27966      WI      JO                                    It got up there\n",
       "27967      WI     CAM                                   I don't remember\n",
       "27968      WI    WESS  Tie a string around your tooth and then close ...\n",
       "27969      WI      JO                                               Hx H\n",
       "27970      WI     CAM                    Oh I think that was just a joke\n",
       "27971      WI      JO                                 H SM I think so SM\n",
       "27972      WI    WESS                                                 Mm\n",
       "27973      WI      JO                     Yeah I never saw anybody do it\n",
       "27974      WI    WESS                                    Old wive's tale\n",
       "27975      WI     CAM                   Yeah ENV ROCKING_CHAIR_SQUEAKING\n",
       "27976      WI      JO                                              Aw me\n",
       "27977      WI     CAM         Your chair's a little squeaky X Jo Lynne X\n",
       "27978      WI      JO                                               GASP\n",
       "27979      WI    WESS                        Not a creature was stirring\n",
       "27980      WI      JO                                        Well oil it\n",
       "27981      WI     CAM                                                NaN\n",
       "27982      WI      JO                         H It's a getting to be old\n",
       "27983      MI    FRED                            You did bring the gifts\n",
       "27984      WI     CAM                              I did bring the gifts\n",
       "\n",
       "[27985 rows x 3 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Load all transcription .csv data into one DataFrame\n",
    "'''\n",
    "\n",
    "directory = \"../../data/SBData/dialectsdata\"\n",
    "transcripts = pd.DataFrame({\"name\":[], \"dialect\":[], \"word\":[]})\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".csv\") and filename.startswith(\"dialects\"): \n",
    "        currpath = os.path.join(directory, filename)\n",
    "        currdata = pd.read_csv(currpath, names=[\"name\", \"dialect\", \"word\"])\n",
    "        transcripts = pd.concat([transcripts, currdata])\n",
    "        continue\n",
    "    else:\n",
    "        continue\n",
    "transcripts = transcripts.reset_index()\n",
    "transcripts = transcripts.drop(\"index\", axis = 1)\n",
    "transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Clean transcriptions DataFrame\n",
    "'''\n",
    "SBexamples = []\n",
    "SBtargets = []\n",
    "\n",
    "for i,x in enumerate(transcripts['word']):\n",
    "    if pd.notnull(x) and pd.notnull(transcripts.get_value(i, 'dialect')):\n",
    "        try:\n",
    "            dialect = transcripts.get_value(i,'dialect')\n",
    "            dialect = abbrev_to_state(dialect)\n",
    "            target = catagories.index(dialect)\n",
    "            SBexamples.append(x)\n",
    "            SBtargets.append(target) \n",
    "        except KeyError:\n",
    "            # These speakers do not identify with a single state, discard.\n",
    "            pass\n",
    "# Sanity check: print(len(SBexamples), len(SBtargets)) should be equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Bunching the data based on parameters\n",
    "'''\n",
    "\n",
    "# THIS WILL BE A PARAMETER IN THE FUNCTION, if true, then combine\n",
    "\n",
    "## ANNOYING BUG: the transcriptions data is heavily imbalanced, so it tends to classify everything as cali, ma, etc.\n",
    "combine_param = True\n",
    "dare_only=False\n",
    "sb_only=False\n",
    "\n",
    "alltargets = []\n",
    "allexamples = []\n",
    "if combine_param:\n",
    "    alltargets.extend(targets)\n",
    "    allexamples.extend(examples)\n",
    "    alltargets.extend(SBtargets)\n",
    "    allexamples.extend(SBexamples)\n",
    "    training = sklearn.datasets.base.Bunch(target=alltargets, data=allexamples, target_names=catagories)\n",
    "elif dare_only:\n",
    "    training = sklearn.datasets.base.Bunch(target=targets, data=examples, target_names=catagories)\n",
    "elif sb_only:\n",
    "    training = sklearn.datasets.base.Bunch(target=SBtargets, data=SBexamples, target_names=catagories)\n",
    "else:\n",
    "    print(\"Unacceptable parameter input\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to classification (both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gooselock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>swale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tickbird</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>twistification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ahkio</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "0       gooselock\n",
       "1           swale\n",
       "2        tickbird\n",
       "3  twistification\n",
       "4           ahkio"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = pd.DataFrame(training.data)\n",
    "training_df.astype('U').values.ravel()\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58250, 12477)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Vectorize text data by the number of occurances (count)\n",
    "'''\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(training_df.astype('U').values.ravel())\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4720)\t1.0\n",
      "  (1, 10863)\t1.0\n",
      "  (2, 11238)\t1.0\n",
      "  (3, 11578)\t1.0\n",
      "  (4, 200)\t1.0\n",
      "  (5, 739)\t1.0\n",
      "  (6, 830)\t1.0\n",
      "  (7, 836)\t1.0\n",
      "  (8, 1055)\t1.0\n",
      "  (9, 1084)\t1.0\n",
      "  (10, 1917)\t1.0\n",
      "  (11, 2041)\t1.0\n",
      "  (12, 5244)\t1.0\n",
      "  (13, 5315)\t1.0\n",
      "  (14, 5326)\t1.0\n",
      "  (15, 5486)\t1.0\n",
      "  (16, 6011)\t1.0\n",
      "  (17, 6041)\t1.0\n",
      "  (18, 6120)\t1.0\n",
      "  (19, 6199)\t1.0\n",
      "  (20, 6438)\t1.0\n",
      "  (21, 6660)\t1.0\n",
      "  (22, 6770)\t1.0\n",
      "  (23, 7146)\t1.0\n",
      "  (24, 7156)\t1.0\n",
      "  :\t:\n",
      "  (58243, 1843)\t0.415302549754\n",
      "  (58243, 5909)\t0.462627651073\n",
      "  (58243, 10487)\t0.512550109597\n",
      "  (58244, 4543)\t1.0\n",
      "  (58245, 7514)\t0.315331798893\n",
      "  (58245, 11965)\t0.272204673193\n",
      "  (58245, 2626)\t0.63096247611\n",
      "  (58245, 10628)\t0.654489744946\n",
      "  (58246, 12048)\t0.462297754016\n",
      "  (58246, 5795)\t0.360174346453\n",
      "  (58246, 7617)\t0.810280955465\n",
      "  (58247, 11291)\t0.325176247785\n",
      "  (58247, 5795)\t0.299692966023\n",
      "  (58247, 912)\t0.412870642384\n",
      "  (58247, 4620)\t0.570586550057\n",
      "  (58247, 7634)\t0.555349759651\n",
      "  (58248, 12421)\t0.25398614638\n",
      "  (58248, 11122)\t0.258984706352\n",
      "  (58248, 3067)\t0.387372425413\n",
      "  (58248, 1402)\t0.534658123562\n",
      "  (58248, 4628)\t0.657648275495\n",
      "  (58249, 11122)\t0.267765304679\n",
      "  (58249, 3067)\t0.40050587147\n",
      "  (58249, 1402)\t0.552785133034\n",
      "  (58249, 4628)\t0.679945133981\n"
     ]
    }
   ],
   "source": [
    "'''Normalize vectorized data with Term Frequency times Inverse Document Frequency (tfidf)\n",
    "'''\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print(X_train_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Train a Naive Bayes classifier on the existing data\n",
    "'''\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB().fit(X_train_tfidf, training.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "california\n",
      "6\n",
      "california\n",
      "6\n",
      "california\n",
      "6\n",
      "california\n",
      "['california', 'california', 'california', 'california']\n"
     ]
    }
   ],
   "source": [
    "'''Example of utilizing the NB classifier to predict the geographic origin of untrained data points.\n",
    "'''\n",
    "testing = ['What a gooselock!', \n",
    "            'Scoot your tush over.', \n",
    "            \"That's a hosey. Don't barf up your frappe.\", \n",
    "            \"gosh hm Leah She snoozing on the floor\"]\n",
    "X_testing_counts = count_vect.transform(testing)\n",
    "\n",
    "X_testing_tfidf = tfidf_transformer.transform(X_testing_counts)\n",
    "\n",
    "predicted = classifier.predict(X_testing_tfidf)\n",
    "res = []\n",
    "for doc, category in zip(testing, predicted):\n",
    "    print(category)\n",
    "    print(training.target_names[category])\n",
    "    res.append(training.target_names[category])\n",
    "    \n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
